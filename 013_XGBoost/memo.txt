chapter 1 環境構築
　XGBoostのインストール
　・Anaconda Navigatorからインストール
　・conda install -c anaconda py-xgboost でも可。

chapter 2
2.1. XGBoostとは
　・XGBoostとは「決定木を用いた勾配ブースティング」のオープンソースのフレームワーク。

2.2. アンサンブル学習
　・勾配ブースティング（英：Gradient boosting）とは、回帰と分類問題に使われる機械学習の手法の一つ。
　・複数の「決定木」の弱学習器を一つにまとめて予測を行う。
　・勾配ブースティングにおいて使われる弱学習器は決定木に限らないが、一般的には決定木を用いたものが多い。
　
　アンサンブル学習
　　・英語でアンサンブル（Ensemble）といえば「合唱」や「合奏」を意味。
　　・機械学習においてのアンサンブル学習（Ensemble Learning）は、複数のモデル（弱学習器）を融合させて一つの学習モデルを生成する手法。
　　・分類では多数決、回帰では平均をとることが多い。
　　・「バギング」「ブースティング」「スタッキング」の３つの手法に分類される。
　　・「勾配ブースティング」はアンサンブル学習の「ブースティング」に属する手法。
　　・「シングル」は一つの学習器（モデル）を使った分類。
　　・学習データ
　　　・「シングル」の場合は訓練データとして用意されている「全部」を使いモデル訓練を行う。
　　　・「バギング」では訓練データからサンプルを抽出して、それぞれの学習器が異なる訓練データの学習を行う。
　　　・「ブースティング」は最初に学習した結果を、次の学習データに反映をさせる。一つ前のモデルの「誤差」を利用。
　　・処理の流れ
　　　・「バギング」は「並列処理」。全ての弱学習器が独立しており、それぞれの弱学習器の結果を最終的にまとめる。
　　　・ブースティングは「順次処理」。それぞれの弱学習器が関わり合いながら学習を繰り返す。一つ前の弱学習器の推測結果の「誤差」を次の学習データに反映して新しい弱学習器を訓練する。
　　・複数の弱学習器を一つにまとめる手法を「アンサンブル学習」という。
　　・「スタッキング」について
　　　・学習器の出力結果を次の学習の特徴量として利用する手法。

2.3. 勾配ブースティングとは
　・加法モデリング（英：Additive Modeling）
　　・複数のモデルを加法（足していき）一つのモデルとする手法。
　・残差
　　・残差を段階を重ねながら学習する。
　・勾配ブースティングの流れ
　　・残差とは実際の正解ラベルからモデルの推測結果の差分。
　　・１つめの弱学習器を構築し残差を求める。モデル１の残差を使用して弱学習器２は学習を実施、残差を弱学習器３が使用・・

2.4. 勾配ブースティング
　・numpy.full(shape, fill_value, dtype=None, order='C')：任意の値で配列を生成し初期化
　　
ステップ2 モデルと予測（ステージ2）
　・特徴量：広告費、ターゲット：ステージ１の残差として、決定木(DecisionTreeRegressor())で学習。
　・広告費を入力にしてステージ１の残差を予測するように訓練する。
　・ステージ１での推測値（売上）にステージ２の推測値（誤差）を加算したものを、ステージ２の推測値（売上）。

　・pd.concat([table, stage_2], axis=1)：データフレームの連結

ステップ5 データを拡張して確認
　・np.random.uniform(0, 9, 20)：一様乱数を発生。0以上9未満で20個。
　・np.concatenate((y1, y2, y3, y4, y5))：numpy 配列の結合
　・mean_squared_error(y, y_pred)：MSE（平均2乗和誤差）
　・mean_absolute_error(y, y_pred)：MAE（平均絶対誤差）


chapter 3 XGBoostの基本操作
　データセット：Iris
　・特徴量：
　　・sepal length:がく片の長さ(cm)
　　・sepal width:がく片の幅(cm)
　　・petal length:花弁の長さ(cm)
　　・petal width:花弁の幅(cm)
　・ターゲット：
　　・Iris-Setosa:アイリスセトサ
　　・Iris-Versicolour:アイリスバージカラー
　　・Iris-Virginica:アイリスバージニカ
　・データ数：150

3.3. モデル訓練と評価
　・DMatrix（読み：ディー・マトリックス）というデータ構造を使用。
　・DMatrixを用いることでメモリ効率と訓練スピードを最適化することが可能。
　・XGBoost のハイパーパラメータ
　　・max_depth : 決定木の最大の深さ
　　・eta : 学習率
　　・silent : 訓練中のメッセージ表示オプション（0=表示）
　　・objective : 訓練タスクの目的
　　・num_class : ターゲットのクラス数
　・xgb.train(param, d_train, num_boost_round)：XGBoostのモデル訓練
　　・num_boost_round：反復回数。デフォルトは 10。
　・xgboost_model.predict(d_train)：推測
　・np.argmax(y_prob_train[0])：一番確率のインデクスを取得
　
3.4. Scikit-learn API
　・Scikit-learn APIでは、Numpy配列のままモデル訓練を行うことが可能。
　・DMatrix型ではなくて numpy 配列で訓練可能。
　・pandasのDataFrameでも訓練できるが、警告が出る。
　・pd_DataFrame.vales：numpy 配列
　・反復回数は n_estimators で、デフォルトは 100。

