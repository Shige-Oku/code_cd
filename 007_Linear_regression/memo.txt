chapter 3 機械学習

・機械学習の定義は明確に定まっておらず長年議論されている
・アルゴリズムを使用してデータを解析し、学習をして予測を行う（Nvidia）
・明示的にプログラミングをすることなく動作させる科学（Stanford)
・例から一般化することによって重要なタスクを実行する方法(Washington)
・人工知能は1950年代から研究されている大きな分野
・人工知能も明確な定義は定まっておらず広い意味で使われている
・機械学習は人工知能という大きな分野の一つ


chapter 4 教師あり学習

・教師あり学習とは、学習データに正解ラベルを付けて学習する方法
・最高気温32度で11.2万円の売り上げ（正解ラベル）などのような事例を学習する
・予測最高気温が30度であれば売り上げの予測＝回帰問題
・身長152cmである場合、女性か男性かの予測＝分類問題


chapter 5 教師なし学習

・教師なし学習とは、学習データに正解ラベルを付けないで学習する方法
・大量の写真を特徴ごとに分類する＝クラスタリングの事例


chapter 6

・線形回帰は英語でLinear Regression（リニアー・レグレッション）
・線形回帰とは「複数の変数における相関関係を直線モデルによって説明する分析手法」
・「当日の最高気温」から「アイスクリームの売り上げ」を予測する例題
・最高気温＝特徴量、説明変数、独立変数（複数の呼び名がある）
　：ターゲットを説明する変数
・売り上げ＝ターゲット、目的変数、従属変数（複数の呼び名がある）
　：予測したい変数
・2つのデータの関係性を表す直線＝線形モデル
・1つの特徴量からターゲットを予測する＝単回帰分析
・複数の特徴量からターゲットを予測する＝重回帰分析
・線形モデルの数式 ：^y = w1x + w0
・^y = ターゲット（ワイ・ハット）
・w1 = 重み（weight）、係数（Coefficient）
・w0 = 定数項（Constant）、Y切片（y-intercept）
・x = 特徴量


chapter 7 線形回帰モデル

・線形モデルとは異なる複数のデータの関係性を最もらしく表す直線
・訓練データとは正解ラベルを含む「事例」。教師データとも呼ばれる。
・訓練データの個数 = 49名 = m
・特徴量 = 勉強時間 = x
・実際のテスト点数 = y
・モデルが予測したテスト点数（ターゲット） = ^y（ワイ・ハット）
・訓練データ：教師データ
　　学習に使用しデータに潜在する特徴を捉え、未知の入力に対して予測する。
・線形回帰モデル
　訓練データ => 学習アルゴリズム => モデル
　^y = w1x + w0


chapter 8 コスト関数

・線形モデルの数式：^y = w1x + w0
・w1（傾き / Slope）とw0（Y切片 / Constant / y-intercept）を2つ合わせて線形モデルのパラメーターと呼ぶ
・w1（傾き）とw0（Y切片）の値で線形モデル（直線）が変わる
・データに対して最も適しているw1とw0＝データに最もフィットした直線
・残差 / コスト/ 誤差 ＝ 予測値 ^y から実際値 y の差
・コストが最小となる直線 = データに最もフィットしている直線
・残差の求め方
　J(w0, w1) = 1 / 2m * 残差の2乗の総和（二乗誤差）

chapter 9 コストとパラメーター

・線形モデルの数式：^y = w1x + w0
・コスト関数：J(w0, w1) = 1 / 2m * ∑(^yi ? yi) ** 2
・パラメーターの値（w1、w0）、直線、コストの関係性


chapter 10 コスト関数とグラフ

・線形モデルの数式：^y = w1x + w0
・コスト関数、損失関数：J(w0, w1) = 1 / 2m * ∑(^yi - yi) ** 2 二乗誤差
・モデル式のパラメーター（w1とw0）の最適な値の求め方
・w1, w0, 誤差の３次元で考える


chapter 11 最急降下法

w1, w1のパラメータを最適化する。
誤差を最小化する。

・最急降下法は英語でGradient Descent（グラディエント・ディセント）
・最急降下法の役割はパラメーター（w1とw0）の値を更新
・最急降下法の数式：w0, w1でそれぞれ偏微分したもので同時更新
　　w0 := w0 - α * ∂J(w0, w1) / ∂w0 （コスト関数をw0で偏微分し勾配を求める）

　　w1 := w1 - α * ∂J(w0, w1) / ∂w1（コスト関数をw1で偏微分し勾配を求める）
　　
　　α：学習率


chapter 11 h偏微分と学習率

・最急降下法の数式：
  w0 := w0 - α * ∂J(w0, w1) / ∂w0
  
  w1 := w1 - α * ∂J(w0, w1) / ∂w1
・α = 学習率
・∂J(w0,w1) / ∂w1 = 偏微分
・偏微分は傾きを計算して、w1とw0を正しい方向で更
・学習率は正の値を持ち、w1とw0の1回の更新の大きさを決める
・学習率が大きすぎるとw1とw0の最適な値へ辿り着かない可能性がある

学習率が小さいと学習に時間がかかる。
学習率が小さいと局所的最適解に囚われる可能性がある。
学習率が大きいと誤差の最小個所を超えてしまい収束しない可能性がある。


chapter 13 コスト関数と最急降下法

・最急降下法の数式：

  w0 := w0 - α * ∂J(w0, w1) / ∂w0

  w1 := w1 - α * ∂J(w0, w1) / ∂w1

・偏微分を計算した最急降下法の数式：

  w0 := w0 - α * 1 / m ∑m i=1(^yi - yi)

  w1 := w1 - α * 1 / m ∑m i=1(^yi - yi) * xi

上段も下段も同じ最急降下法の式。
最急降下法は（w1とw0）を同時更新する。
モデル、コスト関数、最急降下法の役割。


chapter 14 単回帰分析

・特徴量が1つの単回帰分析
・生徒のテスト勉強時間からテストの点数を予測
・特徴量（x）＝テスト勉強時間、ターゲット（y）＝テスト点数


chapter 15 データの読み込みと確認

・データは49名分の生徒のデータ
・np.loadtxt()関数でCSVファイルをNumpy配列として読み込む
・data[:, 0]は0行目でx、data[:, 1]は1行目でy


chapter 16 コスト関数の作成

・線形モデルの数式：^y = w1 * x + w0
・コスト関数：J(w0, w1) = 1 / 2m * ∑m i=1(^yi - yi) ** 2
・コスト関数（^yを代入）：J(w0, w1)=1 / 2m * ∑m i=1((w1 * x + w0) - yi) ** 2

・len(data) = データの個数（m = 生徒49名）
・コードのどの部分が計算式のどの部分に該当しているのか把握
・w0, w1= 0, 0（w1とw0の初期値を0とする）


chapter 17 最急降下法の作成

最急降下法の数式：

　w0 := w0 - α * 1 / m *∑m i=1(^yi - yi)

　w1 := w1 - α * 1 / m * ∑m i=1(^yi - yi) * xi

w0_in、w1_inは最急降下法へ入力するパラメーター
w0_out、w1_outは最急降下法が処理を行い更新したパラメーター


chapter 18 線形回帰モデル構築

alpha = 学習率(0.01)
int_w0 = w0の初期値(0）
int_w1 = w1の初期値(0）
iterations = 学習回数(5000)


chapter 19 重回帰分析とモデル

単回帰 モデル式（特徴量 xが1つ）

　^y = w1 * x + w0

重回帰 モデル式（特徴量 xが２つ）

　^y = w1 * x1 + w2 * x2 + w0 * x0

　x0=1

モデル式のベクトル化

　^y = X * W

モデル式のベクトル化＝行列を利用して計算を行う

　X = 大文字Xは特徴量の行列

　W = 大文字Wはパラメーターの行列


chapter 20 コスト関数と最急降下法

線形回帰 モデル数式

  ^y = X * W = w1 * x1 + w2 * x2 ... + wn * xn + w0 * x0(1)

コスト関数（単回帰）

  J(w0, w1) = 1 / 2m * ∑m i=1(^yi - yi) ** 2

コスト関数（ベクトル化）^y = XW を代入

  J(W) = 1 / 2m (XW - y)T * (XW - y)

最急降下法（単回帰）

  w0 := w0 - α * 1 / m * ∑m i=1(^yi - yi)

  w1 := w1 - α * 1 / m * ∑m i=1(^yi - yi) * xi

最急降下法（ベクトル化）

  W := W - α * 1 / m XT * (XW - y)


chapter 21 正規化

正規化とは「特徴量のスケールを調整する」こと。
英語では「Feature Scaling（フューチャー・スケーリング）」または「Normalization（ノーマライゼイション）」。

・最急降下法がより効率よく最適化ポイントを探すことができる
・各特徴量の尺度を調整するテクニック
・正規化には複数の計算方法がある
・厳密に考えなくても特徴量の尺度があっていればOK

標準化（Z-socre Normalization）
平均を 0 、標準偏差を 1にする

  x1 = (x1 - xmean) / std

min-max normalization
最大値が1, 最小値が 0になるようにする 0 <= X =< 1 

  x1 = (x1 - xmin) /  (xmax - xmin)


chapter 22 重回帰分析

・ 物理、化学、統計のテスト点数から数学の点数を予測したい
・ 特徴量の数(n)＝3（物理、化学、統計）
・ データのサイズ(m)＝99（生徒99名）
・ ターゲット(y)＝数学の点数


chapter 23 特徴量の正規化

標準化（Z-score Normalization）

  x1 = (x1 - xmean) / std

・stdとはStandard Deviationの略で「標準偏差」
・標準偏差とは「データのばらつき具合を表す指標」
・標準化ではxの平均を0、標準偏差を1となるような処理を加える
・np.mean() = 平均値を取得
・np.std() = 標準偏差を取得


chapter 24 重回帰分析のコスト関数

モデル式（ベクトル化）

 ^y = X * W

コスト関数（重回帰）

 J(W) = 1 / 2m * ∑m i=1(^yi - yi) ** 2

コスト関数（ベクトル化）

 J(W) = 1 / 2m * (XW - y)T * (XW - y)

・m : データの長さ（99名の生徒）
・^y : ターゲット（予測する数学のテスト点数）
・X : 特徴量の行列（物理、化学、統計の点数の行列）
・W : パラメーターの行列（w0?w3の行列）


chapter 25 重回帰分析の最急降下法

線形回帰 モデル数式

  ^y = X * W = w1 * x1 + w2 * x2 ... + wn * xn + w0 * x0

最急降下法（ベクトル化）

 W := W - α * 1 / m * XT * (XW - y)

・α = 学習率（最急降下法の歩幅を設定する値）
・m = データのサイズ（生徒99名）
・X = 特徴量の行列（99行、4列）
・W = パラメーターの行列（4行、1列）
・Iterations = 学習回数（最急降下法の計算の反復回数）
・J_history = 最急降下法の各計算回数毎のコスト履歴


chapter 26 モデルを使って予測

訓練済みのモデル式

 ^y = 2.229 * (物理点数正規化) + 5.453 * (化学点数正規化) + 3.220 * (統計点数正規化) + 74.577

 w0=74.577, w1=2.229, w2=5.453, w3=3.220

・各特徴量が正規化されている部分に注意
・予測する場合は同等の正規化の処理をする必要がある



