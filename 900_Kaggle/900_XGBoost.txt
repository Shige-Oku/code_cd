
XGBoostのハイパーパラメータ

XGBoost Parameters - xgboost 1.1.0-SNAPSHOT documentation
https://xgboost.readthedocs.io/en/latest/parameter.html

XGBoostパラメータのまとめとランダムサーチ実装 - Qiita
https://qiita.com/FJyusk56/items/0649f4362587261bd57a


・XGBoostを実行する前に、一般パラメータ、ブースターパラメータ、タスクパラメータの3種類のパラメータを設定する。
　・一般的なパラメータは、どのブースターを使用してブースティングを行うか、通常はツリーまたは線形モデルに関連。
　・ブースターパラメータは、選択したブースターによって異なる。
　・学習タスクのパラメータは学習シナリオを決定する。
　　回帰タスクは、ランキングタスクとは異なるパラメータを使用することがある。
　・コマンドラインパラメータはXGBoostのCLIバージョンの動作に関連している。

・XGBBoostのハイパーパラメータは３つの種類に分類。
　①「一般的なパラメータ（General Parameters）」：
　　・XGBoostのブースティングの方法を指定するパラメータ。
　　・決定木（gbtree）や線形モデル(gblinear)を設定することが可能。
　②「ブースターパラメータ（Booster Parameters）」：
　　・ブースティングのより細かい設定を行うパラメータ
　③「学習タスクパラメータ（Learning Task Parameters）」
　　・XGBoostの学習目標（learning objective）を設定する。

General Parameters
　・booster [default= gbtree ]
　　使用するブースター。 gbtree、gblinear、またはdartを使用できる。
　　gbtreeとdartはツリーベースのモデルを使用し、gblinearは線形関数を使用。

　・verbosity [default=1]
　　表示メッセージの強弱を指定。有効な値は 0 (サイレント)、1 (警告)、2 (情報)、3 (デバッグ) 。
　　XGBoostはヒューリスティックに基づいて設定を変更しようとするが、これは警告メッセージとして表示される。
　　予期せぬ挙動がある場合は、冗長度の値を大きくしてみる。

　・validate_parameters [default to false, except for Python interface]
　　Trueに設定すると、XGBoostは入力パラメータの検証を行い、パラメータが使用されているかどうかを確認する。
　　この機能はまだ実験段階。特にScikit-Learnインターフェースで使用した場合は、多少の誤検出がある。

　・nthread [default to maximum number of threads available if not set]
　　XGBoostの実行に使用する並列スレッド数

　・disable_default_eval_metric [default=0]
　　デフォルトのメトリックを無効にするフラグ。0以上に設定すると無効になる。

　・num_pbuffer [set automatically by XGBoost, no need to be set by user]
　　予測バッファのサイズ、通常は学習インスタンスの数に設定される。
　　バッファは、最後のブーストステップの予測結果を保存するために使用されます。

　・num_feature [set automatically by XGBoost, no need to be set by user]
　　ブースティングに使用される特徴量の次元数。

Parameters for Tree Booster
　・eta [default=0.3, alias: learning_rate]
　　更新時に使用されるステップサイズの幅。オーバーフィットを防ぐために使用されます。
　　各ブースティングステップの後、新しい機能の重みを直接取得でき、etaは機能の重みを縮小して、
　　ブースティングプロセスをより保守的にします。
　　range: [0,1]
　　・learning_rate：
　　　・XGBoostの学習率を設定。
　　　・勾配ブースティングの各ステージで特徴量の重みを縮小させる度合いの調整を行う値。
　　　・デフォルトは0.3。

　・gamma [default=0, alias: min_split_loss]
　　木のリーフノード上でさらに分割を行うのに必要な最小の損失削減．
　　ガンマが大きいほど，アルゴリズムはより保守的になります．
　　range: [0,∞]
　　・枝分かれに必要な最低限の損失関数の減少を決める値。
　　・Gammaの値が大きいとアルゴリズムはより保守的に働く。

　・max_depth [default=6]
　　木の最大深度。この値を大きくするとモデルが複雑になり、オーバーフィットしやすくなります。
　　0は、木の深さに制限がないことを示し、 tree_methodがhistに設定されている場合にのみ、
　　ロスガイド成長ポリシーで受け入れられます。
　　XGBoostは深い木を学習する際にメモリを積極的に消費するので注意してください。
　　range: [0,∞] (0 is only accepted in lossguided growing policy when tree_method is set as hist)
　　・決定木の最大の深さを設定する値。
　　・一般的な値として3～10が多い。
　　・max_depthを調整することで過学習（Overfitting）をコントロールする事が可能

　・min_child_weight [default=1]
　　子ノードに必要なインスタンスの重みの最小和（ヘシアン）。
　　ツリー分割ステップの結果、インスタンス重みの合計がmin_child_weightよりも小さいリーフノードになった場合、
　　構築プロセスはそれ以上の分割をあきらめます。
　　線形回帰タスクでは、これは単に各ノードに必要なインスタンスの最小数に対応します。
　　min_child_weightが大きいほど、アルゴリズムはより保守的になります。
　　range: [0,∞]
　　・子ノードに置ける必要な最小の重み。
　　・決定木が枝分かれする際に、ノードの重み（Weight）の合計値がmin_child_weightより小さい場合は枝分かれをし、
　　　それ以上は行わない。
　　・過学習をコントロールするハイパーパラメータで、この値を高くすると特徴量の局所的な学習を防げる。
　　・値が高すぎると逆に未学習（Underfitting)になる。

　・max_delta_step [default=0]
　　各リーフ出力に許可する最大のデルタステップ。
　　この値が0に設定されている場合、制約がないことを意味します。
　　正の値に設定すると、更新ステップをより保守的にするのに役立ちます。
　　通常このパラメータは必要ありませんが、クラスが非常に不均衡な場合にロジスティック回帰を行う際に役立つかもしれません。　　1-10の値に設定すると、更新を制御するのに役立つかもしれません。
　　range: [0,∞]

　・subsample [default=1]
　　学習インスタンスのサブサンプル比。
　　これを0.5に設定すると、XGBoostは木を成長させる前にトレーニングデータの半分をランダムにサンプリングします。
　　となり、オーバーフィットを防ぐことができます。
　　サブサンプリングは、ブースティングのイテレーションごとに1回発生します。
　　range: (0,1]
　　・XGBoostで構築される決定木に使われるデータを制御する。
　　・勾配ブースティンの各ステージで訓練データの一部を使うようコントロールする。
　　・subsampleが0.5の場合は、各ステージで訓練データの「半分」をランダムに選択して訓練を行う。
　　・subsampleの値が低いとアルゴリズムはより保守的になり過学習を防ぐことが可能。

　・sampling_method [default= uniform]
　　学習インスタンスのサンプリングに使用するメソッド。
　　uniform: 
　　　各トレーニング・インスタンスが選択される確率が等しい。
　　　通常、良好な結果を得るために、subsample >= 0.5 を設定します。
　　gradient_based: 
　　　各学習インスタンスの選択確率は、正則化された勾配の絶対値に比例します
　　　（より具体的には、g2+λh2-------√）  (g2 + λh2）** (1 / 2)。
　　　subsample は，モデルの精度を損なうことなく，0.1まで低く設定することができます．
　　　このサンプリング方法は tree_method が gpu_hist に設定されている場合にのみサポートされることに注意してください。
　　　他のツリーメソッドはuniformサンプリングのみをサポートしています。

　・colsample_bytree, colsample_bylevel, colsample_bynode [default=1]
　　これは、列のサブサンプリングのためのパラメータのファミリーです。
　　すべての colsample_by* パラメータの範囲は (0, 1], デフォルト値は 1 で、サブサンプリングする列の割合を指定します。
　　colsample_bytree は、各ツリーを構築する際の列のサブサンプル率です。
　　　サブサンプリングは、構築された各ツリーに対して 1 回行われます。
　　colsample_bylevel は、各レベルの列のサブサンプル比率です。
　　　サブサンプリングは、ツリー内の新しい深さレベルに到達するごとに 1 回発生します。
　　　カラムは、現在のツリーで選択されたカラムのセットからサブサンプリングされます。
　　colsample_bynode は、各ノード (分割) のカラムのサブサンプル比です。
　　　サブサンプリングは、新しいスプリットが評価されるたびに 1 回行われます。
　　　カラムは、現在のレベルで選択されたカラムのセットからサブサンプリングされます。
　　colsample_by* パラメータは累積的に動作します。
　　　例えば、{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}と
　　　64個のフィーチャの組み合わせは、各スプリットで8個のフィーチャから選択できるようになります。
　　・colsample_bytree ：
　　　・XGBoostの各ステージで使われる特徴量の制御を行う。
　　　・値が低いとアルゴリズムは保守的になるため過学習を防ぐ効果がある。
　　　・デフォルトは1。

　・lambda [default=1, alias: reg_lambda]
　　重みに対するL2正則化項。
　　この値を大きくするとモデルはより保守的になります。
　　・reg_lambda：
　　　・L2正則化（英：L2 Regularization）の重みを制御する。
　　　・過学習を調整。
　　　・デフォルトは0。

　・alpha [default=0, alias: reg_alpha]
　　重みに対するL1正則化項。
　　この値を大きくするとモデルはより保守的になります。
　　・reg_alpha：
　　　・L1正則化（英：L1 Regularization）の重みを制御する。
　　　・値を高く設定すると過学習を防ぐ効果がある。
　　　・デフォルトは0。

　・tree_method string [default= auto]
　　・XGBoostで使用されているツリー構築アルゴリズム。
　　・XGBoostは分散学習のために approx, hist, gpu_hist をサポートしています。
　　　外部メモリの実験的なサポートは approx と gpu_hist に対応しています。
　　・選択肢: auto, exact, approx, hist, gpu_hist, これは、一般的に使用されるアップデータの組み合わせです。
　　　リフレッシュのような他のアップデータについては、パラメータのアップデータを直接設定してください。
　　　・auto:
　　　　ヒューリスティックを使用して最速の方法を選択します。
　　　　・小規模なデータセットでは，e-greedy アルゴリズム（厳密）が使用される．
　　　　・大規模なデータセットでは、近似アルゴリズム(approximate algorithm)が選択されます。
　　　　　大規模なデータセットでは、histやgpu_histを使ってみることをお勧めします。
　　　　　(gpu_hist)は外部メモリをサポートしています。
　　　　・従来の動作では、単一マシンでは常にe-greedy を使用していたため、
　　　　　近似アルゴリズムが選択されるとメッセージが表示され、その旨が通知されるようになっています。
　　　・exact: 
　　　　e-greedyアルゴリズム。分割されたすべての候補を列挙します。
　　　・approx:
　　　　分位点スケッチと勾配ヒストグラムを使用した近似貪欲アルゴリズム。
　　　・hist:
　　　　より高速なヒストグラム最適化された近似貪欲アルゴリズム．
　　　・gpu_hist:histアルゴリズムのGPU実装．

　・sketch_eps [default=0.03]
　　・tree_method=approx でのみ使用されます。
　　・これはおおまかにO（1 / sketch_eps）個のビンに変換されます。
　　　ビンの数を直接選択する場合と比較すると、これはスケッチの精度を理論的に保証するものです。
　　・通常、ユーザーはこれを調整する必要はありません。
　　　ただし、分割候補をより正確に列挙するには、より低い数に設定することを検討してください。
　　range: (0, 1)

　・scale_pos_weight [default=1]
　　正と負の重みのバランスを制御します。これは、不均衡なクラスに役立ちます。
　　考慮すべき典型的な値：sum（negative instances）/ sum（positive instances）。
　　詳細については、パラメーターのチューニングを参照してください。
　　また、例については、Higgs Kaggle競争デモを参照してください：R、py1、py2、py3。

　・updater [default= grow_colmaker,prune]
　　・実行するツリーアップデーターのシーケンスを定義するコンマ区切りの文字列。
　　・ツリーを構築および変更するためのモジュール方式を提供します。
　　・これは、他のいくつかのパラメーターに応じて、通常は自動的に設定される拡張パラメーターです。
　　・ただし、ユーザーが明示的に設定することもできます。
　　・次のアップデータが存在します。
　　　・grow_colmaker: 
　　　　木の非分散列ベースの構築。
　　　・grow_histmaker：
　　　　ヒストグラムカウントのグローバルな提案に基づく、行ベースのデータ分割による分散ツリー構築。
　　　・grow_local_histmaker：
　　　　ローカルヒストグラムカウントに基づく。
　　　・grow_skmaker: 
　　　　近似スケッチアルゴリズムを使用します。
　　　・grow_quantile_histmaker：
　　　　量子化ヒストグラムを使用してツリーを成長させます。
　　　・grow_gpu_hist: 
　　　　GPU を使ってツリーを成長させます。
　　　・sync: 
　　　　すべての分散ノードのツリーを同期させます。
　　　・refresh:
　　　　現在のデータに基づいて、ツリーの統計情報やリーフ値を更新します。
　　　　データ行のランダムなサブサンプリングは実行されないことに注意してください。
　　　・prune:
　　　　損失<min_split_loss（またはガンマ）である分割を枝刈りします。
　　・分散設定では、暗黙のアップデータシーケンス値はデフォルトでgrow_histmaker、pruneに調整され、
　　　grow_histmakerを使用するようにtree_methodを履歴として設定できます。
　　・refresh_leaf [default=1]
　　　これはリフレッシュアップデータのパラメータです。
　　　このフラグが1の場合、ツリーノードの統計情報だけでなく、ツリーリーフの統計情報も更新されます。
　　　0の場合は、ノードの統計情報のみが更新されます。

　・esh_leaf [default=1]
　　実行するツリーのアップデータのシーケンスを定義するカンマ区切りの文字列で、
　　ツリーを構築したり変更したりするモジュール式の方法を提供します。 
　　これは高度なパラメータで、通常は他のパラメータに応じて自動的に設定されます。
　　ただし、ユーザーが明示的に設定することもできます。
　　以下のアップデータが存在します。
　　　・grow_colmaker: 
　　　　非分散カラムベースの木の構築
　　　・grow_histmaker:
　　　　ヒストグラムカウントのグローバルな提案に基づく、行ベースのデータ分割による分散ツリー構築。
　　　・grow_local_histmaker：
　　　　ローカルヒストグラムカウントに基づいています。
　　　・grow_skmaker：
　　　　近似スケッチアルゴリズムを使用します。
　　　・grow_quantile_histmaker：
　　　　量子化されたヒストグラムを使用してツリーを成長させます。
　　　・grow_gpu_hist：
　　　　GPUでツリーを成長させます。
　　　・sync：
　　　　すべての分散ノードのツリーを同期します。
　　　・refresh: 
　　　　現在のデータに基づいて、ツリーの統計や葉の値を更新します。 
　　　　データ行のランダムなサブサンプリングは実行されないことに注意してください。
　　　・prune：
　　　　損失<min_split_loss（またはガンマ）である分割を枝刈りします。
　　分散設定では、暗黙のアップデータシーケンス値はデフォルトでgrow_histmaker、pruneに調整され、
　　grow_histmakerを使用するようにtree_methodを履歴として設定できます。

　・refresh_leaf [default=1]
　　リフレッシュアップデータのパラメータです。
　　このフラグが 1 の場合、ツリーリーフとツリーノードの統計情報が更新されます。
　　0 の場合は、ノードの統計情報のみが更新されます。

　・process_type [default= default]
　　・実行するブースティングプロセスの一種。
　　・Choices: default, update
　　　・default: 
　　　　新しいツリーを作成する通常のブースティングプロセス。
　　　・update: 
　　　　既存のモデルから開始し、そのツリーのみを更新します。
　　　　各ブースティング反復では、初期モデルからツリーが取得され、
　　　　そのツリーに対して指定された一連のアップデーターが実行され、変更されたツリーが新しいモデルに追加されます。
　　　　新しいモデルでは、実行されたブースティングイテラトンの数に応じて、ツリーの数が同じか、それより少なくなります。
　　　　現在のところ、以下の組み込みアップデータをこのプロセス・タイプで使用することができます: refresh、prune。
　　　　process_type=update の場合、新しいツリーを作成するアップデータは使用できません。

　・grow_policy [default= depthwise]
　　・新しいノードがツリーに追加される方法を制御します。
　　・現在のところ tree_method が hist に設定されている場合のみサポートされています。
　　・Choices: depthwise, lossguide
　　　・depthwise: 
　　　　ルートに最も近いノードで分割します。
　　　・lossguide:
　　　　 最も損失の変化が大きいノードで分割します。

　・max_leaves [default=0]
　　・tree_method が hist に設定されている場合にのみ使用されます。
　　・連続特徴量をバケツに入れる離散ビンの最大数。（離散値の特徴量を分割する数、範囲）
　　・この数を増やすと、計算時間が長くなりますが、スプリットの最適性が向上します。

　・predictor, [default=``auto``]
　　・使用する予測アルゴリズムのタイプ。
　　　同じ結果が得られますが、GPUまたはCPUを使用できます。
　　　・auto：
　　　　ヒューリスティックに基づいて予測子を構成します。
　　　・cpu_predictor：
　　　　マルチコアCPU予測アルゴリズム。
　　　・gpu_predictor：
　　　　tree_methodがgpu_histの場合に使用されます。
　　　　予測子がデフォルト値autoに設定されている場合、gpu_histツリーメソッドは、
　　　　トレーニングデータをGPUメモリにコピーすることなく、GPUベースの予測を提供できます。
　　　　gpu_predictorが明示的に指定されている場合、すべてのデータがGPUにコピーされるので、
　　　　予測タスクの実行にのみ推奨されます。

　・num_parallel_tree, [default=1] 
　　各反復の間に構築される並列木の数。
　　このオプションは、ブーストされたランダムフォレストをサポートするために使用されます。

　・monotone_constraints
　　可変単調性の制約。 詳細については、チュートリアルを参照してください。

　・interaction_constraints
　　許可された相互作用を表す相互作用の制約。
　　制約は、 [[0、1]、[2、3、4]]のようにネストリストの形式で指定する必要があります。 。
　　ここで、各内部リストは、相互に対話することが許可されている機能のインデックスのグループです。
　　詳細については、チュートリアルを参照してください

Additional parameters for gpu_hist tree method
　・single_precision_histogram, [default=``false``]
　　単精度を使用してヒストグラムを作成します。
　　詳細については、GPUサポートのドキュメントを参照してください。

　・deterministic_histogram, [default=``true``]
　　GPUでヒストグラムを確定的に構築します。
　　ヒストグラムの作成は、浮動小数点加算の非関連性の側面により、確定的ではありません。
　　問題を軽減するために事前丸めルーチンを採用しているため、わずかに精度が低下する可能性があります。
　　無効にするにはfalseに設定します。

Additional parameters for Dart Booster (booster=dart)
　DART ブースターでの predict() の使用
　ブースターオブジェクトがDART型の場合、predict()はドロップアウトを実行します。
　つまり、一部の木のみが評価されます。
　これは、データが学習データではない場合、誤った結果を生成します。
　テストセットで正しい結果を得るには、ntree_limitを0以外の値に設定します。
　preds = bst.predict(dtest, ntree_limit=num_round)

　・sample_type [default= uniform]
　　サンプリングアルゴリズムの種類。
　　　uniform:
　　　　ドロップされたツリーは均一に選択されます。
　　　weighted:
　　　　落とされた木は重量に比例して選択されます。

　・normalize_type [default= tree]
　　正規化アルゴリズムのタイプ。
　　・tree:
　　　新しいツリーは、ドロップされた各ツリーと同じ重みを持ちます。
　　　・新しい木の重みは1 /（k + learning_rate）です。
　　　・ドロップされたツリーは、k /（k + learning_rate）の係数でスケーリングされます。
　　forest: 
　　　新しい木は、落とされた木（森）の合計と同じ重みを持ちます。
　　　・新しい木の重みは1 /（1 + learning_rate）です。
　　　・ドロップされたツリーは、1 /（1 + learning_rate）の係数でスケーリングされます。

　・rate_drop [default=0.0]
　　・ドロップアウト率（ドロップアウト中にドロップする前の木の割合）。
　　・range: [0.0, 1.0]

　・one_drop [default=0]
　　このフラグを有効にすると、ドロップアウト中に少なくとも1つのツリーが常にドロップされます（
　　元のDARTペーパーからのBinomial-plus-oneまたはepsilon-dropoutが許可されます）。

　・skip_drop [default=0.0]
　　・ブースティング反復中にドロップアウト手順をスキップする確率。
　　　・ドロップアウトをスキップすると、gbtreeと同じ方法で新しいツリーが追加されます。
　　　・ゼロ以外のskip_dropは、rate_dropまたはone_dropよりも優先度が高いことに注意してください。
　　・range: [0.0, 1.0]

Parameters for Linear Booster (booster=gblinear)
　・lambda [default=0, alias: reg_lambda]
　　重みに対するL2正則化項。
　　この値を大きくするとモデルがより保守的になる。
　　学習例の数に正規化されている。

　・alpha [default=0, alias: reg_alpha]
　　重みに対する L1 正則化項。
　　この値を大きくすると，モデルはより保守的になる．
　　学習例の数に正規化される．


　・updater [default= shotgun]
　　・線形アルゴリズムに適合するアルゴリズムの選択
　　　・shotgun：
　　　　ショットガンアルゴリズムに基づく並列座標降下アルゴリズム。
　　　　「hogwild」並列処理を使用するため、実行ごとに非決定的なソリューションが生成されます。
　　　・coord_descent：
　　　　通常の座標降下アルゴリズム。
　　　　また、マルチスレッド化されていますが、確定的なソリューションを提供します。

　・feature_selector [default= cyclic]
　　特徴の選択と順序付け方法
　　・cyclic: 
　　　一度に1つの特徴を循環させることによる決定論的選択。
　　・shuffle: 
　　　サイクリックに似ていますが、各更新前にランダムな特徴シャッフルを行います。
　　・random：
　　　ランダム（置換あり）の座標セレクター。
　　・greedy: 
　　　勾配の大きさが最も大きい座標を選択する．
　　　O(num_feature^2) の複雑さを持つ。
　　　完全に決定論的である。
　　　top_kパラメータを設定することで，一変量の重みの変化の大きさが最大のグループごとの特徴量を
　　　top_kに制限することができます．
　　　そうすることで，複雑さを O(num_feature*top_k) に減らすことができます．
　　・thrifty: 
　　　倹約的な、ほぼ貪欲な特徴セレクター。
　　　周期的な更新の前に，一変量の重みの変化の大きさの降順で特徴を並べ替えます．
　　　この操作はマルチスレッドで行われ，二次的貪欲選択の線形複雑度近似です．
　　　これにより、top_kパラメータを設定することで、一変量の重みの変化の大きさが最大のグループごとの特徴量を
　　　top_kに制限することができます。

　・top_k [default=0]
　　・貪欲で倹約的な特徴セレクターで選択する上位特徴の数。値0は、すべての機能を使用することを意味します。

Parameters for Tweedie Regression (objective=reg:tweedie)
　・tweedie_variance_power [default=1.5]
　　・トゥイーディー分布の分散を制御するパラメーター
　　　var(y) ~ E(y)^tweedie_variance_power
　　・range: (1,2)
　　・ガンマ分布にシフトするには、2に近く設定します。
　　・ポアソン分布にシフトするには、1に近く設定します。

Learning Task Parameters
　・学習タスクと対応する学習目的を指定します。
　　学習目標のオプションは以下の通りです。

　・objective [default=reg:squarederror]
　　・reg：squarederror：
　　　二乗損失を伴う回帰。
　　・reg:squaredlogerror: 
　　　二乗対数損失12 [log（pred + 1）-log（label + 1）] 2を使用した回帰。
　　　すべての入力ラベルは-1より大きい必要があります。
　　　また、この目的で考えられる問題については、メトリックrmsleを参照してください。
　　・reg：logistic：
　　　ロジスティック回帰
　　・binary:logistic: 
　　　二値分類のためのロジスティック回帰、出力確率
　　・binary:logitraw: 
　　　二値分類のためのロジスティック回帰、ロジスティック変換前の出力スコア
　　・binary:hinge: 
　　　二値分類のためのヒンジ損失。これは確率を出すのではなく、0か1かの予測をします。
　　・count:poisson 
　　　カウントデータのポアソン回帰，ポアソン分布の出力平均
　　　・max_delta_stepは、ポアソン回帰ではデフォルトで0.7に設定されています（最適化を保護するために使用されます）。
　　・survival:cox. 
　　　右打ち切り生存時間データのCox回帰（負の値は右打ち切りとみなされる）．
　　　予測値はハザード比スケール（すなわち，比例ハザード関数 h(t) = h0(t) * HRのHR = exp(marginal_prediction)として）
　　　で返されることに注意してください．
　　・survival：aft：
　　　打ち切り生存時間データの加速故障時間モデル。
　　　詳細については、加速故障時間を伴う生存分析を参照してください。
　　・aft_loss_distribution：
　　　survival：aftおよびaft-nloglikメトリックで使用される確率密度関数。
　　・multi：softmax：
　　　softmax目標を使用してマルチクラス分類を行うようにXGBoostを設定します。num_class（クラス数）も設定する
　　　必要があります。
　　・multi：softprob：
　　　softmaxと同じですが、ndata * nclassのベクトルを出力します。これはさらにndata * nclass行列に再形成できます。
　　　結果には、各クラスに属する各データポイントの予測確率が含まれます。
　　・rank:pairwise:
　　　LambdaMARTを使用して，ペアワイズの損失が最小になるようなペアワイズの順位付けを行います．
　　・rank：ndcg：
　　　LambdaMARTを使用して、正規化割引累積ゲイン（NDCG）が最大化されているリストごとのランキングを実行します
　　・rank：map：
　　　LambdaMARTを使用して、平均平均精度（MAP）が最大化されるリストごとのランキングを実行します
　　・reg:gamma: log-linkを用いたガンマ回帰。
　　　出力は，ガンマ分布の平均値です．
　　　これは，例えば，保険金請求の重症度のモデル化や，ガンマ分布に従う可能性のある結果に役立つ場合があります。
　　・reg：tweedie：
　　　ログリンクを使用したTweedie回帰。
　　　これは、たとえば、保険の全損失をモデル化する場合や、結果がTweedie分布に従う場合に役立つことがあります。

　・base_score [default=0.5]
　　・すべてのインスタンスの初期予測スコア、グローバルバイアス
　　・十分な反復回数の場合、この値を変更してもあまり効果はありません。

　・eval_metric [default according to objective]
　　・検証データの評価指標は、目的に応じてデフォルトの指標が割り当てられます
　　（回帰の場合はrmsse、分類の場合は誤差、ランキングの場合は平均平均精度）。
　　・ユーザーは複数の評価指標を追加できます。
　　　Pythonユーザー：指標をマップではなくパラメーターのペアのリストとして渡すことを忘れないでください。
　　　これにより、後者のeval_metricが以前の指標をオーバーライドしない
　　・選択肢は次のとおりです。
　　　・rmse：
　　　　二乗平均平方根誤差
　　　・rmsle：
　　　　二乗平均平方根エラー
　　　　( N / 1 (log(pred+1)-log(label+1)) ** 2 ) ** 1/2
　　　　reg:squaredlogerror objectiveのデフォルトのメトリック．
　　　　このメトリックはデータセットの外れ値によって発生する誤差を軽減します。
　　　　ただし、log関数を採用しているため、予測値が-1よりも小さい場合、rmsleはnanを出力することがあります。
　　　　他の要件についてはreg:squaredlogerrorを参照してください。
　　　・mae：
　　　　平均絶対誤差
　　　・logloss:
　　　　 負の対数尤度
　　　・error: 
　　　　二値分類の誤り率。
　　　　#(間違ったケース)/#(全ケース)で計算される。
　　　　予測値については、予測値が0.5より大きいものを正のインスタンス、それ以外を負のインスタンスとして評価する。
　　　・error@t: 
　　　　t で数値を指定することで、0.5 以外の 2値分類のしきい値を指定することができる。
　　　・merror：
　　　　マルチクラス分類エラー率。
　　　　＃（間違ったケース）/＃（すべてのケース）として計算されます。
　　　・mlogloss：
　　　　マルチクラスのログロス。
　　　・auc：
　　　　曲線の下の面積
　　　・aucpr：
　　　　PR曲線の下の面積
　　　・ndcg：
　　　　正規化された割引累積ゲイン
　　　・マップ：
　　　　平均平均精度
　　　・ndcg @ n、map @ n：
　　　　「n」を整数として割り当て、評価のためにリストの最上位を切り捨てることができます。
　　　・ndcg-、map-、ndcg @ n-、map @ n-：
　　　　XGBoostでは、NDCGとMAPは、正のサンプルを含まないリストのスコアを1と評価します。
　　　　評価メトリックに「-」を追加することにより、XGBoostはこれらのスコアを0として評価し、
　　　　特定の条件下で一貫性を保ちます。
　　　・poisson-nloglik: 
　　　　ポアソン回帰の負の対数尤度
　　　・gamma-nloglik: 
　　　　ガンマ回帰の負の対数尤度
　　　・cox-nloglik：
　　　　Cox比例ハザード回帰の負の部分対数尤度
　　　・gamma-deviance:：
　　　　ガンマ回帰の残差
　　　・tweedie-nloglik：
　　　　Tweedie回帰の負の対数尤度（tweedie_variance_powerパラメーターの指定値で）
　　　・aft-nloglik:
　　　　Accelerated Failure Time モデルの負の対数尤度。詳細は、
　　　　加速化された故障時間による生存率分析を参照してください。

　・seed [default=0]
　　乱数シード。
　　このパラメーターはRパッケージでは無視されます。代わりにset.seed（）を使用してください。

Command Line Parameters
　次のパラメーターは、XGBoostのコンソールバージョンでのみ使用されます。
　・num_round：
　　ブースティングのラウンド数

　・data
　　トレーニングデータのパス

　・test:data
　　予測を行うためのテストデータのパス

　・save_period [default=0]
　　モデルを保存する期間。
　　save_period = 10を設定すると、10ラウンドごとにXGBoostがモデルを保存します。
　　0に設定すると、トレーニング中にモデルが保存されません。

　・task [default= train] options: train, pred, eval, dump
　　・train：
　　　データを使用したトレーニング
　　・pred：
　　　test：dataの予測を行う
　　・eval: 
　　　eval[name]=filename で指定された統計情報を評価するためのもの
　　・dump：
　　　学習したモデルをテキスト形式にダンプします

　・model_in [default=NULL]
　　テスト、eval、ダンプタスクに必要な入力モデルへのパス。
　　トレーニングで指定された場合、XGBoostは入力モデルからトレーニングを継続します。

　・model_out [default=NULL]
　　トレーニング終了後の出力モデルへのパス。
　　指定しない場合、XGBoostは0003.modelなどの名前のファイルを出力します。0003はブーストラウンドの数です。

　・model_dir [default= models/]
　　トレーニング中に保存されたモデルの出力ディレクトリ

　・fmap
　　モデルのダンプに使用されるフィーチャーマップ

　・dump_format [default= text] options: text, json
　　モデルダンプファイルのフォーマット

　・name_dump [default= dump.txt]
　　モデルダンプファイル名

　・name_pred [default= pred.txt]
　　predモードで使用される予測ファイルの名前

　・pred_margin [default=0]
　　変換された確率の代わりにマージンを予測


