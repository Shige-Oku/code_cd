
XGBoostのハイパーパラメータ

XGBoost Parameters - xgboost 1.1.0-SNAPSHOT documentation
https://xgboost.readthedocs.io/en/latest/parameter.html

XGBoostパラメータのまとめとランダムサーチ実装 - Qiita
https://qiita.com/FJyusk56/items/0649f4362587261bd57a


・XGBoostを実行する前に、一般パラメータ、ブースターパラメータ、タスクパラメータの3種類のパラメータを設定する。
　・一般的なパラメータは、どのブースターを使用してブースティングを行うか、通常はツリーまたは線形モデルに関連。
　・ブースターパラメータは、選択したブースターによって異なる。
　・学習タスクのパラメータは学習シナリオを決定する。
　　回帰タスクは、ランキングタスクとは異なるパラメータを使用することがある。
　・コマンドラインパラメータはXGBoostのCLIバージョンの動作に関連している。

・XGBBoostのハイパーパラメータは３つの種類に分類。
　①「一般的なパラメータ（General Parameters）」：
　　・XGBoostのブースティングの方法を指定するパラメータ。
　　・決定木（gbtree）や線形モデル(gblinear)を設定することが可能。
　②「ブースターパラメータ（Booster Parameters）」：
　　・ブースティングのより細かい設定を行うパラメータ
　③「学習タスクパラメータ（Learning Task Parameters）」
　　・XGBoostの学習目標（learning objective）を設定する。

General Parameters
　・booster [default= gbtree ]
　　使用するブースター。 gbtree、gblinear、またはdartを使用できる。
　　gbtreeとdartはツリーベースのモデルを使用し、gblinearは線形関数を使用。

　・verbosity [default=1]
　　表示メッセージの強弱を指定。有効な値は 0 (サイレント)、1 (警告)、2 (情報)、3 (デバッグ) 。
　　XGBoostはヒューリスティックに基づいて設定を変更しようとするが、これは警告メッセージとして表示される。
　　予期せぬ挙動がある場合は、冗長度の値を大きくしてみる。

　・validate_parameters [default to false, except for Python interface]
　　Trueに設定すると、XGBoostは入力パラメータの検証を行い、パラメータが使用されているかどうかを確認する。
　　この機能はまだ実験段階。特にScikit-Learnインターフェースで使用した場合は、多少の誤検出がある。

　・nthread [default to maximum number of threads available if not set]
　　XGBoostの実行に使用する並列スレッド数

　・disable_default_eval_metric [default=0]
　　デフォルトのメトリックを無効にするフラグ。0以上に設定すると無効になる。

　・num_pbuffer [set automatically by XGBoost, no need to be set by user]
　　予測バッファのサイズ、通常は学習インスタンスの数に設定される。
　　バッファは、最後のブーストステップの予測結果を保存するために使用されます。

　・num_feature [set automatically by XGBoost, no need to be set by user]
　　ブースティングに使用される特徴量の次元数。

Parameters for Tree Booster
　・eta [default=0.3, alias: learning_rate]
　　更新時に使用されるステップサイズの幅。オーバーフィットを防ぐために使用されます。
　　各ブースティングステップの後、新しい機能の重みを直接取得でき、etaは機能の重みを縮小して、
　　ブースティングプロセスをより保守的にします。
　　range: [0,1]
　　・learning_rate：
　　　・XGBoostの学習率を設定。
　　　・勾配ブースティングの各ステージで特徴量の重みを縮小させる度合いの調整を行う値。
　　　・デフォルトは0.3。

　・gamma [default=0, alias: min_split_loss]
　　木のリーフノード上でさらに分割を行うのに必要な最小の損失削減．
　　ガンマが大きいほど，アルゴリズムはより保守的になります．
　　range: [0,∞]
　　・枝分かれに必要な最低限の損失関数の減少を決める値。
　　・Gammaの値が大きいとアルゴリズムはより保守的に働く。

　・max_depth [default=6]
　　木の最大深度。この値を大きくするとモデルが複雑になり、オーバーフィットしやすくなります。
　　0は、木の深さに制限がないことを示し、 tree_methodがhistに設定されている場合にのみ、
　　ロスガイド成長ポリシーで受け入れられます。
　　XGBoostは深い木を学習する際にメモリを積極的に消費するので注意してください。
　　range: [0,∞] (0 is only accepted in lossguided growing policy when tree_method is set as hist)
　　・決定木の最大の深さを設定する値。
　　・一般的な値として3～10が多い。
　　・max_depthを調整することで過学習（Overfitting）をコントロールする事が可能

　・min_child_weight [default=1]
　　子ノードに必要なインスタンスの重みの最小和（ヘシアン）。
　　ツリー分割ステップの結果、インスタンス重みの合計がmin_child_weightよりも小さいリーフノードになった場合、
　　構築プロセスはそれ以上の分割をあきらめます。
　　線形回帰タスクでは、これは単に各ノードに必要なインスタンスの最小数に対応します。
　　min_child_weightが大きいほど、アルゴリズムはより保守的になります。
　　range: [0,∞]
　　・子ノードに置ける必要な最小の重み。
　　・決定木が枝分かれする際に、ノードの重み（Weight）の合計値がmin_child_weightより小さい場合は枝分かれをし、
　　　それ以上は行わない。
　　・過学習をコントロールするハイパーパラメータで、この値を高くすると特徴量の局所的な学習を防げる。
　　・値が高すぎると逆に未学習（Underfitting)になる。

　・max_delta_step [default=0]
　　各リーフ出力に許可する最大のデルタステップ。
　　この値が0に設定されている場合、制約がないことを意味します。
　　正の値に設定すると、更新ステップをより保守的にするのに役立ちます。
　　通常このパラメータは必要ありませんが、クラスが非常に不均衡な場合にロジスティック回帰を行う際に役立つかもしれません。　　1-10の値に設定すると、更新を制御するのに役立つかもしれません。
　　range: [0,∞]

　・subsample [default=1]
　　学習インスタンスのサブサンプル比。
　　これを0.5に設定すると、XGBoostは木を成長させる前にトレーニングデータの半分をランダムにサンプリングします。
　　となり、オーバーフィットを防ぐことができます。
　　サブサンプリングは、ブースティングのイテレーションごとに1回発生します。
　　range: (0,1]
　　・XGBoostで構築される決定木に使われるデータを制御する。
　　・勾配ブースティンの各ステージで訓練データの一部を使うようコントロールする。
　　・subsampleが0.5の場合は、各ステージで訓練データの「半分」をランダムに選択して訓練を行う。
　　・subsampleの値が低いとアルゴリズムはより保守的になり過学習を防ぐことが可能。

　・sampling_method [default= uniform]
　　学習インスタンスのサンプリングに使用するメソッド。
　　uniform: 
　　　各トレーニング・インスタンスが選択される確率が等しい。
　　　通常、良好な結果を得るために、subsample >= 0.5 を設定します。
　　gradient_based: 
　　　各学習インスタンスの選択確率は、正則化された勾配の絶対値に比例します
　　　（より具体的には、g2+λh2-------√）  (g2 + λh2）** (1 / 2)。
　　　subsample は，モデルの精度を損なうことなく，0.1まで低く設定することができます．
　　　このサンプリング方法は tree_method が gpu_hist に設定されている場合にのみサポートされることに注意してください。
　　　他のツリーメソッドはuniformサンプリングのみをサポートしています。

　・colsample_bytree, colsample_bylevel, colsample_bynode [default=1]
　　これは、列のサブサンプリングのためのパラメータのファミリーです。
　　すべての colsample_by* パラメータの範囲は (0, 1], デフォルト値は 1 で、サブサンプリングする列の割合を指定します。
　　colsample_bytree は、各ツリーを構築する際の列のサブサンプル率です。
　　　サブサンプリングは、構築された各ツリーに対して 1 回行われます。
　　colsample_bylevel は、各レベルの列のサブサンプル比率です。
　　　サブサンプリングは、ツリー内の新しい深さレベルに到達するごとに 1 回発生します。
　　　カラムは、現在のツリーで選択されたカラムのセットからサブサンプリングされます。
　　colsample_bynode は、各ノード (分割) のカラムのサブサンプル比です。
　　　サブサンプリングは、新しいスプリットが評価されるたびに 1 回行われます。
　　　カラムは、現在のレベルで選択されたカラムのセットからサブサンプリングされます。
　　colsample_by* パラメータは累積的に動作します。
　　　例えば、{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}と
　　　64個のフィーチャの組み合わせは、各スプリットで8個のフィーチャから選択できるようになります。
　　・colsample_bytree ：
　　　・XGBoostの各ステージで使われる特徴量の制御を行う。
　　　・値が低いとアルゴリズムは保守的になるため過学習を防ぐ効果がある。
　　　・デフォルトは1。

　・lambda [default=1, alias: reg_lambda]
　　重みに対するL2正則化項。
　　この値を大きくするとモデルはより保守的になります。
　　・reg_lambda：
　　　・L2正則化（英：L2 Regularization）の重みを制御する。
　　　・過学習を調整。
　　　・デフォルトは0。

　・alpha [default=0, alias: reg_alpha]
　　重みに対するL1正則化項。
　　この値を大きくするとモデルはより保守的になります。
　　・reg_alpha：
　　　・L1正則化（英：L1 Regularization）の重みを制御する。
　　　・値を高く設定すると過学習を防ぐ効果がある。
　　　・デフォルトは0。

　・tree_method string [default= auto]
　　・XGBoostで使用されているツリー構築アルゴリズム。
　　・XGBoostは分散学習のために approx, hist, gpu_hist をサポートしています。
　　　外部メモリの実験的なサポートは approx と gpu_hist に対応しています。
　　・選択肢: auto, exact, approx, hist, gpu_hist, これは、一般的に使用されるアップデータの組み合わせです。
　　　リフレッシュのような他のアップデータについては、パラメータのアップデータを直接設定してください。
　　　・auto:
　　　　ヒューリスティックを使用して最速の方法を選択します。
　　　　・小規模なデータセットでは，e-greedy アルゴリズム（厳密）が使用される．
　　　　・大規模なデータセットでは、近似アルゴリズム(approximate algorithm)が選択されます。
　　　　　大規模なデータセットでは、histやgpu_histを使ってみることをお勧めします。
　　　　　(gpu_hist)は外部メモリをサポートしています。
　　　　・従来の動作では、単一マシンでは常にe-greedy を使用していたため、
　　　　　近似アルゴリズムが選択されるとメッセージが表示され、その旨が通知されるようになっています。
　　　・exact: 
　　　　e-greedyアルゴリズム。分割されたすべての候補を列挙します。
　　　・approx:
　　　　分位点スケッチと勾配ヒストグラムを使用した近似貪欲アルゴリズム。
　　　・hist:
　　　　より高速なヒストグラム最適化された近似貪欲アルゴリズム．
　　　・gpu_hist:histアルゴリズムのGPU実装．

　・sketch_eps [default=0.03]
　　・tree_method=approx でのみ使用されます。
　　・これはおおまかにO（1 / sketch_eps）個のビンに変換されます。
　　　ビンの数を直接選択する場合と比較すると、これはスケッチの精度を理論的に保証するものです。
　　・通常、ユーザーはこれを調整する必要はありません。
　　　ただし、分割候補をより正確に列挙するには、より低い数に設定することを検討してください。
　　range: (0, 1)

　・scale_pos_weight [default=1]
　　正と負の重みのバランスを制御します。これは、不均衡なクラスに役立ちます。
　　考慮すべき典型的な値：sum（negative instances）/ sum（positive instances）。
　　詳細については、パラメーターのチューニングを参照してください。
　　また、例については、Higgs Kaggle競争デモを参照してください：R、py1、py2、py3。

　・updater [default= grow_colmaker,prune]
　　・実行するツリーアップデーターのシーケンスを定義するコンマ区切りの文字列。
　　・ツリーを構築および変更するためのモジュール方式を提供します。
　　・これは、他のいくつかのパラメーターに応じて、通常は自動的に設定される拡張パラメーターです。
　　・ただし、ユーザーが明示的に設定することもできます。
　　・次のアップデータが存在します。
　　　・grow_colmaker: 
　　　　木の非分散列ベースの構築。
　　　・grow_histmaker：
　　　　ヒストグラムカウントのグローバルな提案に基づく、行ベースのデータ分割による分散ツリー構築。
　　　・grow_local_histmaker：
　　　　ローカルヒストグラムカウントに基づく。
　　　・grow_skmaker: 
　　　　近似スケッチアルゴリズムを使用します。
　　　・grow_quantile_histmaker：
　　　　量子化ヒストグラムを使用してツリーを成長させます。
　　　・grow_gpu_hist: 
　　　　GPU を使ってツリーを成長させます。
　　　・sync: 
　　　　すべての分散ノードのツリーを同期させます。
　　　・refresh:
　　　　現在のデータに基づいて、ツリーの統計情報やリーフ値を更新します。
　　　　データ行のランダムなサブサンプリングは実行されないことに注意してください。
　　　・prune:
　　　　損失<min_split_loss（またはガンマ）である分割を枝刈りします。
　　・分散設定では、暗黙のアップデータシーケンス値はデフォルトでgrow_histmaker、pruneに調整され、
　　　grow_histmakerを使用するようにtree_methodを履歴として設定できます。
　　・refresh_leaf [default=1]
　　　これはリフレッシュアップデータのパラメータです。
　　　このフラグが1の場合、ツリーノードの統計情報だけでなく、ツリーリーフの統計情報も更新されます。
　　　0の場合は、ノードの統計情報のみが更新されます。





　　　　

　　　　



