
・コロモゴロフの確率の公理
　・0 <= p(x) <= 1  確率は非負で1以下
　・Σp(x) = 1  ∫p(x)dx = 1全事象の確率を足し合わせると1
　・p( A ∪ B ) = p(A) + p(B)  確率は足し算できる

　・確率とは不確かさの度合いを表す
　・確率の考え方１：ある事象の考え方 = 着目事象の数 / 全事象の数
　・確率の考え方２：ある事象の確率 = lim(n→∞) na / n  na：ある事象が起こった回数　n：試行回数
　・確率には３つの公理がある。（コロモゴロフの確率の公理）

　・RVS：確率変数
　・事象
　・確率分布
　　確率変数の確率の分布
　・確率質量関数PMF：確率変数が離散的なもの
　・確率密度関数PDF：確率変数が連続的なもの

　・積事象（1回目に３、２回目に偶数）
　・和事象（１または２が出る）
　・余事象（５以外が出る）
　
　・同時確率と条件付き確率
　・独立事象：2つの事象に相関がない
　
　・期待値：分布の中心
　・分散：ばらつきの尺度
　・中心極限定理：多数の系の平均は期待値に近づいていく
　
・ベイズの定理
　θ（原因）、D（結果）とする
　・f(θ, D) = f(D |θ) * f(θ)
　・f(θ, D) = f(θ| D) * f(D)
　・f(θ|D) = f(D|θ) * f(θ) / f(D)
　f(θ|D)：事後分布
　　結果Dが与えられた時の原因θの条件付き確率
　f(D|θ)：尤度関数
　　原因と結果を紐づけている部分（ベイズ統計のメイン）
　f(θ)：事前分布
　　原因についての事前知識の確率分布
　f(D)：エビデンス
　　事後分布の規格化定数
　・ベイズの考え方では出た結果Dを元に、それを発生させる原因θを逆に推定する
　・ベイズの定理により、尤度関数と事前分布、エビデンスから事後分布という逆確率を求める
　・パラメータθの確率分布を考えるかそうでないかという点でベイズ統計と従来の統計学で違う
　・ベイズ統計の流れ
　　１．下調べ：
　　　問題の背景を調べる。原因と結果の関係の仮説を立てる。
　　２．統計モデルの作成：
　　　１．の情報に基づいて統計モデルを作成する。
　　３．事後分布の計算
　　　得られたデータから事後分布を推定する

・from scipy.stats import bernoulli  ベルヌーイ分布
　・bernoulli.pmf(x, p)
　　ベルヌーイ分布
　　x：0 or 1の値
　　p:1の時の確率
　・numpy.prod：行列積を算出
　
・ベルヌーイ分布
　・Bernoulli(x|θ) = θ**x * (1-θ)**(1-x)
　・性質
　　E[X] = θ
　　V[X] = θ(1-θ)
　使用例：コイン投げなど2者択一

・二項分布
　・Binomial(x|N, θ) = N! / (x! * (N-x)!) * θ**x * (1-θ)**(N-x)
　・性質：
　　E[X] = Nθ
　　V[X] = Nθ(1-θ)
　使用例：N回コイン投げしたなど

・一様分布
　・Uniform(x|a, b) = 1 / (b - a)
　・性質
　　E[X] = (b + a) / 2
　　V[X] = (b - a)**2 / 12
　・使用例：事前分ぷや確率変数に特徴がない場合

・Beta分布
　・Beta(x|α,β) = 1 / B(α,β) * x**(α - 1) * (1 - x)**(β - 1)
　・性質
　　E[X] = α / (α + β)
　　V[X] = αβ / (α + β)**2 * (α + β + 1)
　　使用例：[0, 1]で定義される確率変数

・正規分布
　・Normal(x|μ,σ) = 1 / (2 * π * σ**2) * exp( -(x - μ)**2 / 2 * σ**2)
　・性質
　　E[X] = μ
　　V[X] = σ**2
　・使用例：測定誤差や身長の分布など

・指数分布
　・Exponential(x|θ) = 1 / θ * exp(-x / θ)
　　θ：減衰率
　・性質
　　E[X] = θ
　　V[X] = θ**2
　・使用例：機械の耐用年数・地震の起こる間隔など（正規分布より裾が長い）
　・指数分布の無記憶性
　　x' = x + a
　　Exponential(x'|θ) 
　　　=> exp(-x' / θ)
　　　=> exp(-(x + a) / θ)
　　　=> const. * exp(-x / θ)
　　　=> Exponential(x|θ)
　　時間の原点をずらしても分布は同じになる
　　Ex. 昨日事故が起きたから、当分事故は起きないだろうとは限らない（無記憶性)

・Poisson分布
　・Poisson(x|λ) = 1 / x! * λ**x * exp(-λ)
　・性質
　　E[X] = λ
　　V[X] = λ
　・使用例：交通事故の数など（同じ指数分布に従う独立な確率変数のイベント数）

・Gamma分布
　・Gamma(x|k, θ) = 1 / Γ(k) * θ**k * x**(k-1) * exp(-x / θ)
　・性質
　　E[X] = kθ
　　V[X] = k* (θ**2)
　・使用例：感染症の潜伏期間など（K=1の場合、指数分布になる）

・自然共役事分布
　・べき乗同士の積では結果の関数の形が同じ
　　=> 自然共役事前分布の発想
　　　共役とは一緒に出てくるペアのこと
　・自然共役事前分布	尤度関数			事後分布
　　Beta分布			Bernoulli分布		Beta分布
　　Beta分布			2項分布				Beta分布
　　Gamma分布			Poisson分布			Gamma分布
　　正規分布			正規分布の平均		正規分布
　　逆Gamma分布			正規分布の分散		逆Gamma分布
　・統計モデルが決まる⇒尤度関数が決まる⇒自然共役事前分布が決まる
　・事後分布が事前分布と同じ形⇒事後分布が解析的に求まる
　　ベイズ更新している間に事後分布の形がどんどん複雑になる心配がない

・MAP推定・無情報事前分布
　①統計モデルを作る
　　・事後分布の推定
　　　①－１
　　　　自然共役事前分布を使用して手計算
　　　①－２（MAP推定）
　　　　確率分布全体を求めるのは諦める。
　　　　代わりに事後分布が最大の点だけを求める。
　　　①－３
　　　　MCMCで事後分布をサンプリング
　　・f(θ|D)が最大のθを求める = MAP推定（Maximum a Posteriori estimate）
　　　θMAP = argmax(f(θ|D))
　　・事前分布
　　　何も情報がないなら、無情報事前分布
　　　事前分布が無情報事前分布の場合
　　　θMAP = argmax(f(θ|D))
　　　      = argmax(f(D|θ)f(θ))
　　　f(θ) = const.なので
　　　      = const. argmax(f(D|θ))
　　　               最尤推定
　　　MAP推定が最尤推定値に一致（頻度論と整合）
　　　・事前分布は無情報事前分布とする（最尤推定になる）
　　　　θMAP = argmax(f(D|θ)) を変形
　　　　      = argmax(log10 f(D|θ))
　　　　      = argmin(-log10 f(D|θ))
　　　　      = argmin(NLL)
　　　　      NLL:Negative Log Likehood
　　　・NLLをscipyの optmize.minimize を使用して最小化する
　　　　尤度は正規分布に従うと仮定
　　　　f(D|θ) = Normal(D|θ)
　　　　Normal(D|θ) = Π Normal(xi |θ)
　　　　NLL = -log10 f(D|θ)に代入
　　　　    = -log10 ΠNormal(xi |θ)
　　　　    = -Σlog10 Normal(xi|θ)
　　　　               stats.norm.pdf(mu, loc=args)
　・pandas.read_excel()： excel ファイルからデータフレームに読み込み
　【サンプルコード】Pandasのread_excelでExcelファイルを読み込む方法
　　https://yolo.love/pandas/read-excel/
　　pandasでExcelファイル（xlsx, xls）の読み込み（read_excel） | note.nkmk.me
　　https://note.nkmk.me/python-pandas-read-excel/
　  
・モンテカルロ法
　・乱数を使った数値計算手法
　・MCSが増えていくとモンテカルロ法の解が解析解に確率的に漸近していく
　・MCS：Monte Carlo Step　点数をひいた数

　・棄却サンプリング
　　EX α=1.5, β=2.0のBeta分布が事後分布の場合
　　・Beta分布f(x)に従う乱数の発生は諦める。（f(x)を目標分布という）
　　・サンプリングが簡単な一様分布g(x)を使う。（g(x)を提案分布という。一様分布でなくてもよい）
　　・f(x) <= M * g(x) となるように定数Mを調整する
　　　M = max(f(x))
　　
　　・棄却サンプリングのアルゴリズム
　　　①g(x)から乱数で候補xをサンプリング
　　　②Mg(x)から乱数rをひき、次式を判定  r <= f(x)
　　　③真ならばそのxのカウントを1増やす。義ならばそのサンプルは棄却する
　　　④②と③をN回繰り返す
　　　⑤各xでカウント数のヒストグラムを作成

・次元の呪い
　・パラメータが高次元になると対応できない（次元の呪い）
　・的が小さすぎてあたらない。
　・毎回ランダムに点を打つだけだと限界がある。
　・一回一回の試行が独。的が小さいと全く当たらない。

・MCMCと定常分布
　・前の施行の結果を使って次の施行を確率的に改善
　・マルコフ連鎖とは
　　θを確率変数とする
　　θ1⇒θ2⇒・・・⇒θt-1⇒θt のような状態の変化の推移：確率過程
　　この時、新しい状態θtが直前の状態θt-1のみからのみ決定する確率過程（マルコフ性）
　　  f(θt|θt-1,・・・,θ2,θ1) = f(θt|θt-1)：マルコフ連鎖
　　今の条件だけから次の条件が決まる
　・MCMCの流れ
　　①初期値θ0を適当に決める
　　②乱数で今のθから新しいθNewを探す
　　③次の条件式を判定する
　　  f(θnew|D) > f(θ|D)
　　④真であれば状態を更新。義の場合はある程度の確率で受容。
　　⑤②-④を繰り返す
　・遷移核：各場所から各場所へ移る確率
　
・詳細つり合い
　・f(θ'|θ)f(θ) = f(θ|θ')f(θ')
　・移りやすさ　起こりやすさ
　　（遷移核）　（事後分布）
　・遷移核のバリエーション
　　・M-Hアルゴリズム
　　・Gibbsサンプラー（熱浴法）
　　・ハミルトニアンモンテカルロ
　・定常分布に収束するためには確率の流れが釣り合えばよい

・M-Hアルゴリズム（メトロポリスヘイスティングアルゴリズム）
　・遷移核f(θ'|θ)を見つけるのは簡単でない。
　・遷移核に従う乱数を発生させるのも簡単ではない。
　=>
　　サンプリングが簡単な遷移核q(θ'|θ)：提案分布で代用
　　詳細つり合いを満たすように後で補正する
　①初期値θを適当に決める
　②提案分布q(θ'|θ)から乱数をひいて、新しいθ'を探す
　③次の条件式を判定する q(θ'|θ)f(θ) > q(θ|θ')f(θ')
　④真の場合：
　　　θからθへの流れが強い。rを使って、確率的に補正（θ'を引くところまでは発生しているので積事象）
　　　r = q(θ|θ')f(θ') / q(θ'|θ)f(θ)
　　偽の場合：
　　　θからθ'への流れが弱い。詳細つり合いを満たすためには必ずθ'を受け入れる
　⑤②-④を繰り返す
　・ランダムウォークM-H
　　新しいθ'をランダムウォークで決める
　　提案分布がランダムウォークの場合
　　　q(θ|θ') = q(θ'|θ)になる
　　このため、rが事後分布の比になる
　　　r = q(θ|θ')f(θ') / q(θ'|θ)f(θ) = f(θ') / f(θ)
　　①初期値θを適当に決める
　　②現在のθからランダムウォークで新しいθを探してくる
　　　θnew = θ + εNormal(0, 1)
　　③次の条件式を判定する
　　　f(θ) > f(θ')  元の式  q(θ'|θ)f(θ) > q(θ|θ')f(θ')
　　④真の場合：
　　　　確率rで受容し、それ以外は棄却する
　　　　r = f(θ') / f(θ)  元の式  r = q(θ|θ')f(θ') / q(θ'|θ)f(θ)
　　　偽の場合
　　　　θ'を受け入れる
　　⑤②-④を繰り返す
　・バーンイン
　　MCMCではすぐに定常分布になるわけではない
　　MCMCの定常分布へ
　　落ち着くまでの期間を*
　　（*トレースプロットで判断。PyStanではRhatを見る）
　　バーンイン or ウォームアップという
　　事後分布の計算時はバーンインより後で計算する

・Gibbsサンプラー（熱浴法）
　サンプリングしたいパラメータが複数（θ1, θ2, θ3）あったとする
　①θ2、θ3の値を固定（定数とみなす）して、θ1を更新
　　f(θ1|D, θ-1(マイナスがつくとそれ以外) = f(θ1|D, θ2, θ3)
　　                                          Full Conditional分布(FCD)
　②θ1、θ3の値を固定（定数とみなす）して、θ2を更新
　　f(θ2|D, θ-2) = f(θ2|D, θ1, θ3)
　③θ1、θ2の値を固定（定数とみなす）して、θ3を更新
　　f(θ3|D, θ-3) = f(θ3|D, θ1, θ2)

・ハミルトニアンモンテカルロ
　・ランダムウォークM-Hは移動距離が一定になる
　・発想：
　　事後分布の傾き具合で移動距離を変える。（緩やかな個所では小さく、急な個所では大きく）
　・力学
　　物体は運動方程式に従って動く
　　物体にかかる加速度を a、力F、質量mとすると
　　　ma = F  運動方程式
　　加速度
　　　a = dv（増えた速度） / dt （時間）
　　速度
　　　v = dx （移動距離）/ dt （時間）
　　別の解釈
　　　p = mv  運動量
　　　dp / dt = F  運動方程式
　　　ma = m * dv / dt = dp / dt = F
　　・Newton力学の手続き
　　　①物体の運動方程式を立てる dp / dt = F
　　　②運動方程式を解く（解析的 or 数値的）
　　　③任意の時間のx(t)が求まる
　・ポテンシャル
　　重力やクーロン力（電気的な力）など多くの力は次の指揮で表される
　　　  f = - dU(x) / dx 
　　　保存力   ポテンシャル
　　ポテンシャルとは一種の高さと考えることができる
　　ボールを下り坂から上り坂を渡ることを考える
　　①高さ=>速さ（下り）
　　②速さ=>上り（上り）
　　高さ：ポテンシャル　速さ：運動エネルギー
　　エネルギーはなくなっているのではなくて変換されている（エネルギー保存則）
　　力が保存力のみの場合
　　　運動エネルギー + ポテンシャル = 一定　（力学的エネルギー保存の法則）
　　　T = 1 * m * v**2 / 2  運動エネルギー
　　　p = mv なので運動量を使って
　　　T = p**2 / 2m 
　・ハミルトン力学
　　　H =T + U 運動エネルギーとポテンシャルの和
　　　H(q,p) = p**2 / 2m + U(q)
　　　(q,p)により運動を記述する力学の枠組みをハミルトン力学という
　　　上記を一定にするように状態(q, p)が決まる。
　　　物体の運動をq,pの空間で表す（位相空間）
　　・経路の決め方
　　　ハミルトンの正準方程式
　　　　∂H / ∂p = dq / dt
　　　　∂H / ∂q = -dp / dt
　　　を満たすとき、ハミルトニアンは一定値
　　ハミルトニアンが一定になる証明
　　　q, pの関数であるHの時間変化は
　　　　dH(q,p) / dt = ∂H/∂q * dq/dt + ∂H/∂p * dq/dt
　　　ここで正準方程式が成り立つと
　　　　∂H/∂p = dq/dt,  ∂H/∂q = -dp/dt
　　　　dH(q,p)/dt = ∂H/∂q * ∂H/∂p - ∂H/∂p * ∂H/∂q = 0 となり時間変化は0
　　・ハミルトン力学の手続き
　　　①ハミルトニアンを構成する（ポテンシャルが決まれば、校正できる）
　　　　H(q,p) = p**2 / 2m + U(q)
　　　②正準方程式を解く（解析的、数値的に）
　　　　∂H/∂p = dq/dt,  ∂H/∂q = -dp/dt
　　　③(q,p)の経路が位相空間上に決まる
　・ハミルトニアン導出する
　　運動量p
　　Normal(p|0,1)に従うとすると
　　　f(p) ∝ exp(-p**2 / 2)
　　同時分布f(θ,p|D)は
　　　f(θ,p|D) = f(θ|D)f(p)
　　　f = exp(log(f)), f = log(exp(f))
　　という性質を使って
　　　f(θ,p|D) = exp(log f(θ,p|D))
　　　log f(θ,P|D) = log f(θ|D) + log f(p) なので
　　　f(θ,p|D) ∝ exp(log f(θ|D) - p**2 /2)
　　　                 -U(θ)とおく
　　　f(θ,p|D) ∝ exp(-U(θ) - p**2 / 2)
　　　              H = T + U の形になっている
　　　f(θ,p|D) ∝ exp(-H(θ, p))
　　・ハミルトニアンモンテカルロ：アルゴリズム
　　　①初期値θを適当に決める
　　　②pをNormal(p|0,1)から発生させる
　　　③正準法廷岸城に従ってθとpをL回更新
　　　　（Leap-flog法で数値的に解く）
　　　　正準方程式
　　　　∂H/∂p = dθ/dt, ∂H/∂θ = - dp/dt
　　　④次の条件式を判定する
　　　　f(θ',p'|θ,p)f(θ,p|D) > f(θ,p|θ',p')f(θ',p'|D)
　　　⑤真の場合：
　　　　θからθ'への流れが強い。rで確率的に補正
　　　　　r = f(θ',p'|D) / f(θ,p|D) = exp(-H(θ',p')) / exp(-H(θ,p))
　　　　偽の場合：
　　　　　θからθ'への流れが弱い。詳細つり合いを満たすためには必ずθを受け入れる
　　　⑥②-⑤を繰り返す
　　・メリット
　　　・事後分布の傾き具合でpが決まる（移動距離 not= 一定）
　　　・受容確率が極めて高い
　　・デメリット
　　　・傾きが計算できないとダメ（連続量である必要）
　・Euler法とLeaf-flog法
　　・正準方程式を具体的な式として変形し、数値的に離散化することで解ける
　　・数値的に積分する方法として、Euler法とLeap-flog法がある。
　　・Gamma分布が事後分布の場合
　　　Gamma(θ|k,λ) ∝ θ**(k-1) * exp(-λθ) なので
　　　H = p**2 / 2 - log f(θ|D) は
　　　H = p**2 / 2 - (k-1)logθ + λθ
　　　それぞれの偏微分は
　　　　∂H/∂θ = -(k-1)*1/θ + λ
　　　　∂H/∂p = p
　　　正準方程式は
　　　　∂H/∂p = dθ/dt, ∂H/∂θ = -dp/dt
　　　　dθ/dt = p
　　　　dp/dt = (k-1)*1/θ - λ
　　　dtを小さな数で離散化し
　　　　dθ = pdt
　　　　dp = ((k-1)*1/θ - λ)dt
　　　Leap-flog法
　　　　①p(t+0.5) = p(t) + 0.5dp(t)
　　　　②θ(t+1) = θ(t) + dθ(t+0.5)
　　　　③p(t+1) = p(t+0.5) + 0.5dp(t+1)
　　　　 ②
　　　θ====>
　　　p ⇒ ⇒
　　　 ①  ③

・PyStan
　・Stan
　　・HMCを用いたMCMC用のプログラミング言語
　　・特徴
　　　・統計モデルの記述が簡単
　　　・HMCなので高速
　　　・様々な確率分布を用意
　　・確率的プログラミング：統計モデリングに特化したプログラミング言語
　・PyStan
　　・StanをPythonから使えるライブラリ
　　・特徴
　　　・Pythonから動く
　　　・文法はStanに従う
　　　・モデルのコンパイルが必要
　・ベイズの定理
　　f(θ|D)  =  f(θ, D) / f(D) ∝ f(D|θ)f(θ)
　　事後分布    同時分布           尤度   事後分布
　　事後分布は同時分布を（尤度）× （事前分布）に分解することで導かれる
　・グラフィカルモデル
　　因果関係を矢印で表す（有効グラフという）
　　矢印のないグラフィカルモデルもある：無向グラフ
　　・ルール
　　　・矢印はループ構造を持たない
　　　・観測されていない確率変数は塗りつぶさない
　　　・定数など確率変数でないものは点で表す
　　　・観測されている変数は塗りつぶす
　　・グラフィカルモデルのご利益
　　　パラメータ間の因果関係が一目でわかる（複雑なモデルでも理解しやすい）
　　　f(μ,D) = f(D|μ)f(μ)
　　　のように同時分布を分解できる
　　f(θ1, θ2, θ3, θ4, D)の場合　θ１θが決まるとθ３が決まる、θ４が決まるとθ２が決まる場合
　　f(θ1, θ2, θ3, θ4, D) = f(θ3|θ1, D) * f(θ2|θ4, D) * f(θ1) * f(θ4)
　・PyStanでの流れ
　　①モデルの記述・コンパイル
　　　①dataのブロック
　　　②パラメータのブロック
　　　③統計モデルのブロック
　　　④事前分布
　　②データを辞書型で定義
　　③MCMCでサンプリング
　　④結果の抽出

・単回帰
　・仮説
　　物件価格は ax + b と表される
　　y は正規分布している
　・統計モデル
　　y[n] ~ Normal(ax[n] + b, σ)
　　N:物件数
　　a:傾き
　　b:切片
　　x:部屋の大きさ[m**2]
　　y:物件価格[万円]
　　σ:誤差

・ベイズ信頼区間・予測区間
　
・ロジスティック回帰
　入力ファイルの項目
　　・log10 C：薬品投与の対数
　　・death：マウスの生死
　どのくらいの投与量で致死量になるか
　ある閾値を超えると、死亡確率が急激に増加する
　状態は生存／死亡の2値
　・Logistic関数　（シグモイド関数）
　　f(x) = 1 / (1 + exp(-x))
　　PyStanではbernoulli_logit
　　特徴：
　　　f(x) の範囲が[0,1]に納まる
　　　（値が2値の場合に使われやすい）
　　モデルの作成
　　　・仮説
　　　　死亡確率は 1 / (1 + e**-(ax+b)) と表せれる
　　　・統計モデル
　　　　y[n] ~ Bernoulli_logit(ax[n]+b)
　　　　N:検体数
　　　　a:傾き
　　　　b:切片
　　　　x:投与量
　　　　y:生死

・重回帰
　不動産の価格を複数の説明変数から予想する。
　ファイルの項目：
　　・id:物件の固有番号
　　・station:最寄り駅
　　・distance:駅からの距離
　　・space:部屋の大きさ[m**2]
　　・room:間取り
　　・year:建てられた年[year]
　　・type:住宅のタイプ
　　・value:不動産価格[万円]
　事前知識
　　・地区年数は物件の価格にかなり利く
　distance, space, year を説明変数
　value を目的変数
　・統計モデルの作成
　　・仮説：
　　　物件価格は駅からの距離、部屋の大きさ、築年数の線形結合で表せる
　　　　y = dx1 + sx2 + ex3 + b
　　・yは正規分布している
　　・統計モデル
　　　μ[n] = dx1[n] + sx2[n] + ex3[n] + b
　　　y[n] ~ Normal(μ[n], σ)
　　　N:物件数
　　　x1:駅からの距離
　　　x2:大きさ
　　　x3:築年数
　　　d:距離の傾き
　　　s:大きさの傾き
　　　e:築年数の傾き
　　　b:切片
　　　y:物件価格
　　　σ:誤差

・階層ベイズモデル
　・データ
　　age:年齢
　　height:身長[cm]
　　id:生徒の番号
　・事前知識
　　身長の伸びには個人差がある
　　=>階層ベイズが有効
　・単回帰で実装すると。。。
　　仮説：
　　　身長は ax + b と表せる
　　　yは正規分布している
　　前提：
　　　共通の傾きaと切片bに従う
　　　=> 全員が同じ傾きで予測してしまう
　　　N:人数
　　　a:傾き
　　　b:切片
　　　x:年齢
　　　y:身長[cm]
　　　σ:誤差
　・階層ベイズモデル
　　仮説：
　　　身長には個人差があり a[i]x + b[i] と表せる
　　　a[i]  =  a0  +  aid
　　　b[i]  =  b0  +  bid
　　個人毎 共通部分  個人差
　　yと個人差は正規分布している
　　統計モデル
　　　Y[n] ~ Normal(a[n]x + b[n], σ)
　　　a[n] = a0 + aid[n]
　　　b[n] = b0 + bid[n]
　　　aid[n] ~ Normal(0, σa)
　　　bid[n] ~ Normal(0, σb)
　　　N:人数
　　　a[i]:iの傾き
　　　b[i]:iの切片
　　　σa:a[i]の個人差のばらつき
　　　σb:b[i]の個人差のばらつき
　　　x:年齢
　　　y:身長[cm]
　　　σ:誤差
　
・状態空間モデル
　・データ
　　x:時間[年]
　　y:気温の相対値[℃](平均値からの)
　・事前知識
　　気温はある変化幅で推移している
　　=>時系列データには状態空間モデルが有効
　・前提
　　時系列データは何かしらの法則に従って変化している（完全にランダムではない）
　・状態空間モデル
　　次の状態 = 今の状態 + 法則性による変化
　　μ[t+1]  = μ[t]    + ε(t)
　　仮説：
　　　・気温はある法則で時間変化する
　　　・測定気温は正規分布する
　　統計モデル：
　　μ[n] = μ[n-1] + ε[n]
　　ε[n] ~ Normal(0, σμ)
　　 y[n] ~ Noramal(μ[n], σ)
